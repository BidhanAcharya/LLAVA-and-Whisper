{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09-68-WMU_sL"
      },
      "outputs": [],
      "source": [
        "!pip install -q bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbaAI7VwU8Ik",
        "outputId": "774ae3b0-16d4-4688-c721-f8b17c16d991"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsAocJRXSliJ",
        "outputId": "09db16c3-0693-4270-fba2-8a96bc1dd655"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U transformers\n",
        "!pip install -q git+https://github.com/openai/whisper.git\n",
        "!pip install -q gradio\n",
        "!pip install -q gTTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jhftc3tEThgB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BitsAndBytesConfig, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukWF6B3KTwbR"
      },
      "outputs": [],
      "source": [
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTEj4dAkTyYi"
      },
      "outputs": [],
      "source": [
        "model_id = \"llava-hf/llava-1.5-7b-hf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6pIANUoU14i",
        "outputId": "70e162d2-d3fc-4865-c0d1-45f9ad71353c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    print(\"GPU is not available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "WVy7BfpIT0YV",
        "outputId": "0df4dc2f-0f47-4211-ed02-c5b019cdeb73"
      },
      "outputs": [],
      "source": [
        "pipe = pipeline(\"image-to-text\",\n",
        "                model=model_id,\n",
        "                model_kwargs={\"quantization_config\": quantization_config})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPVUdQ-iT8rm"
      },
      "outputs": [],
      "source": [
        "import whisper\n",
        "import gradio as gr\n",
        "import time\n",
        "import warnings\n",
        "import os\n",
        "from gtts import gTTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16IbXJ3sT_kL"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt') ## Downloads the necessary tokenizer model; it’s a setup step.\n",
        "from nltk import sent_tokenize ##Uses the downloaded model to split a given text into sentences; it’s an operational step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaPMCxejUAk1"
      },
      "outputs": [],
      "source": [
        "max_new_tokens = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2KF62_LWuuK"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iroEpnqjWxg3"
      },
      "outputs": [],
      "source": [
        "torch.cuda.is_available()\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using torch {torch.__version__} ({DEVICE})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "notKUXFUW0nZ"
      },
      "outputs": [],
      "source": [
        "import whisper\n",
        "model = whisper.load_model(\"medium\", device=DEVICE)\n",
        "print(\n",
        "    f\"Model is {'multilingual' if model.is_multilingual else 'English-only'} \"\n",
        "    f\"and has {sum(np.prod(p.shape) for p in model.parameters()):,} parameters.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnMOTpmzW2nf"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3gFSWmpXB9z"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import os\n",
        "\n",
        "## Logger file\n",
        "tstamp = datetime.datetime.now()\n",
        "tstamp = str(tstamp).replace(' ','_')\n",
        "logfile = f'{tstamp}_log.txt'\n",
        "def writehistory(text):\n",
        "    with open(logfile, 'a', encoding='utf-8') as f:\n",
        "        f.write(text)\n",
        "        f.write('\\n')\n",
        "    f.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KU3fI3ERXGhp"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import requests\n",
        "from PIL import Image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qg1vpxOiXaCE"
      },
      "outputs": [],
      "source": [
        "def img2txt(input_text, input_image):\n",
        "\n",
        "    # load the image\n",
        "    image = Image.open(input_image)\n",
        "\n",
        "    writehistory(f\"Input text: {input_text} - Type: {type(input_text)} - Dir: {dir(input_text)}\")\n",
        "    if type(input_text) == tuple:\n",
        "        prompt_instructions = \"\"\"\n",
        "        Describe the image using as much detail as possible, is it a painting, a photograph, what colors are predominant, what is the image about?\n",
        "        \"\"\"\n",
        "    else:\n",
        "        prompt_instructions = \"\"\"\n",
        "        Act as an expert in imagery descriptive analysis, using as much detail as possible from the image, respond to the following prompt:\n",
        "        \"\"\" + input_text\n",
        "\n",
        "    writehistory(f\"prompt_instructions: {prompt_instructions}\")\n",
        "    prompt = \"USER: <image>\\n\" + prompt_instructions + \"\\nASSISTANT:\"\n",
        "\n",
        "    outputs = pipe(image, prompt=prompt, generate_kwargs={\"max_new_tokens\": 200})\n",
        "\n",
        "    # Properly extract the response text\n",
        "    if outputs is not None and len(outputs[0][\"generated_text\"]) > 0:\n",
        "        match = re.search(r'ASSISTANT:\\s*(.*)', outputs[0][\"generated_text\"])\n",
        "        if match:\n",
        "            # Extract the text after \"ASSISTANT:\"\n",
        "            reply = match.group(1)\n",
        "        else:\n",
        "            reply = \"No response found.\"\n",
        "    else:\n",
        "        reply = \"No response generated.\"\n",
        "\n",
        "    return reply\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvQpNzy0bZYc"
      },
      "outputs": [],
      "source": [
        "def transcribe(audio):\n",
        "\n",
        "    # Check if the audio input is None or empty\n",
        "    if audio is None or audio == '':\n",
        "        return ('','',None)  # Return empty strings and None audio file\n",
        "\n",
        "    # language = 'en'\n",
        "\n",
        "    audio = whisper.load_audio(audio)\n",
        "    audio = whisper.pad_or_trim(audio)\n",
        "\n",
        "    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
        "\n",
        "    _, probs = model.detect_language(mel)\n",
        "\n",
        "    options = whisper.DecodingOptions()\n",
        "    # options = whisper.DecodingOptions(language='ne')\n",
        "    result = whisper.decode(model, mel, options)\n",
        "    result_text = result.text\n",
        "\n",
        "    return result_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qk2Bx3E6XcWp"
      },
      "outputs": [],
      "source": [
        "def text_to_speech(text, file_path):\n",
        "    language = 'en'\n",
        "\n",
        "    audioobj = gTTS(text = text,\n",
        "                    lang = language,\n",
        "                    slow = False)\n",
        "    \n",
        "\n",
        "    audioobj.save(file_path)\n",
        "\n",
        "    return file_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BwF-VOjXmvU"
      },
      "outputs": [],
      "source": [
        "!ffmpeg -f lavfi -i anullsrc=r=44100:cl=mono -t 10 -q:a 9 -acodec libmp3lame Temp.mp3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RxamCQMXou6"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import base64\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bq842_4mXrx5"
      },
      "outputs": [],
      "source": [
        "def process_inputs(audio_path, image_path):\n",
        "    # Process the audio file (assuming this is handled by a function called 'transcribe')\n",
        "    speech_to_text_output = transcribe(audio_path)\n",
        "\n",
        "    # Handle the image input\n",
        "    if image_path:\n",
        "        LLAVA_output = img2txt(speech_to_text_output, image_path)\n",
        "    else:\n",
        "        LLAVA_output = \"No image provided.\"\n",
        "\n",
        "    # Assuming 'transcribe' also returns the path to a processed audio file\n",
        "    processed_audio_path = text_to_speech(LLAVA_output, \"Temp3.mp3\")  # Replace with actual path if different\n",
        "\n",
        "    return speech_to_text_output, LLAVA_output, processed_audio_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_C7zNWdXuLt"
      },
      "outputs": [],
      "source": [
        "iface = gr.Interface(\n",
        "    fn=process_inputs,\n",
        "    inputs=[\n",
        "        gr.Audio(sources=[\"microphone\"], type=\"filepath\"),\n",
        "        gr.Image(type=\"filepath\")\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Speech to Text\"),\n",
        "        gr.Textbox(label=\"Output of model\"),\n",
        "        gr.Audio(\"Temp.mp3\")\n",
        "    ],\n",
        "    title=\"LLAVA and Whispher implementation\",\n",
        "    description=\"Provide Details about the Image through audio\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "iface.launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34OYLY3hXwwO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
